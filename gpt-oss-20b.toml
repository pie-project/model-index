# Model metadata
name = "gpt-oss-20b"
description = "GPT OSS 20B Model"
version = "1.0"
parameters = ["gpt-oss-20b.zt"]

# Model architecture details
[architecture]
type = "gptoss"
num_layers = 24
num_query_heads = 64
num_key_value_heads = 8
head_size = 64
hidden_size = 2880
intermediate_size = 2880
vocab_size = 201088
use_qkv_bias = true
rms_norm_eps = 1e-5
initial_context_length = 4096
max_position_embeddings = 131072
sliding_window = 128
swiglu_alpha = 1.702
swiglu_beta = 1.0
swiglu_limit = 7.0

[architecture.moe]
num_experts = 32
experts_per_token = 4

[architecture.rope]
theta = 150000.0
scaling_factor = 32.0
ntk_alpha = 1.0
ntk_beta = 32.0

# Tokenizer configuration
[tokenizer]
type = "bpe"
vocabulary_file = "gpt-oss.vocab"
split_regex = "[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
escape_non_printable = true

[tokenizer.special_tokens]
"<|startoftext|>" = 199998
"<|endoftext|>" = 199999
"<|reserved_200000|>" = 200000
"<|reserved_200001|>" = 200001
"<|return|>" = 200002
"<|constrain|>" = 200003
"<|reserved_200004|>" = 200004
"<|channel|>" = 200005
"<|start|>" = 200006
"<|end|>" = 200007
"<|message|>" = 200008
"<|reserved_200009|>" = 200009
"<|reserved_200010|>" = 200010
"<|reserved_200011|>" = 200011
"<|call|>" = 200012
"<|reserved_200013|>" = 200013
"<|reserved_200014|>" = 200014
"<|reserved_200015|>" = 200015
"<|reserved_200016|>" = 200016
"<|reserved_200017|>" = 200017
"<|endofprompt|>" = 200018

# Prompt template configuration
[template]
type = "ninja"
content = """
{#- Simplified Template that Removed Tool Calls to Work With MiniJinja #}
{#- Main Template Logic ================================================= #}
{#- Set defaults #}
{#- Render system message #}
{{- "<|start|>system<|message|>" }}
{%- if model_identity is not defined %}
    {%- set model_identity = "You are ChatGPT, a large language model trained by OpenAI." %}
{%- endif %}
{{- model_identity + "\n" }}
{{- "Knowledge cutoff: 2024-06\n" }}
{{- "Current date: %2025-09-24\n\n" }}
{%- if reasoning_effort is not defined %}
    {%- set reasoning_effort = "medium" %}
{%- endif %}
{{- "Reasoning: " + reasoning_effort + "\n\n" }}
{{- "# Valid channels: analysis, commentary, final. Channel must be included for every message." }}
{{- "<|end|>" }}
{#- Extract developer message #}
{%- if messages[0].role == "developer" or messages[0].role == "system" %}
    {%- set developer_message = messages[0].content %}
    {%- set loop_messages = messages[1:] %}
{%- else %}
    {%- set developer_message = "" %}
    {%- set loop_messages = messages %}
{%- endif %}
{#- Render developer message #}
{%- if developer_message %}
    {{- "<|start|>developer<|message|>" }}
    {%- if developer_message %}
        {{- "# Instructions\n\n" }}
        {{- developer_message }}
        {{- "\n\n" }}
    {%- endif %}
    {{- "<|end|>" }}
{%- endif %}
{#- Render messages #}
{%- for message in loop_messages -%}
    {#- At this point only assistant/user messages should remain #}
    {%- if message.role == 'assistant' -%}
        {#- Checks to ensure the messages are being passed in the format we expect #}
        {%- if "content" in message %}
            {%- if "<|channel|>analysis<|message|>" in message.content or "<|channel|>final<|message|>" in message.content %}
                {{- raise("You have passed a message containing <|channel|> tags in the content field. Instead of doing this, you should pass analysis messages (the string between '<|message|>' and '<|end|>') in the 'thinking' field, and final messages (the string between '<|message|>' and '<|end|>') in the 'content' field.") }}
            {%- endif %}
        {%- endif %}
        {%- if "thinking" in message %}
            {%- if "<|channel|>analysis<|message|>" in message.thinking or "<|channel|>final<|message|>" in message.thinking %}
                {{- raise("You have passed a message containing <|channel|> tags in the thinking field. Instead of doing this, you should pass analysis messages (the string between '<|message|>' and '<|end|>') in the 'thinking' field, and final messages (the string between '<|message|>' and '<|end|>') in the 'content' field.") }}
            {%- endif %}
        {%- endif %}
        {%- if loop.last and not add_generation_prompt %}
            {#- Only render the CoT if the final turn is an assistant turn and add_generation_prompt is false #}
            {#- This is a situation that should only occur in training, never in inference. #}
            {%- if "thinking" in message %}
                {{- "<|start|>assistant<|channel|>analysis<|message|>" + message.thinking + "<|end|>" }}
            {%- endif %}
            {#- <|return|> indicates the end of generation, but <|end|> does not #}
            {#- <|return|> should never be an input to the model, but we include it as the final token #}
            {#- when training, so the model learns to emit it. #}
            {{- "<|start|>assistant<|channel|>final<|message|>" + message.content + "<|return|>" }}
        {%- else %}
            {#- CoT is dropped during all previous turns, so we never render it for inference #}
            {{- "<|start|>assistant<|channel|>final<|message|>" + message.content + "<|end|>" }}
        {%- endif %}
    {%- elif message.role == 'user' -%}
        {{- "<|start|>user<|message|>" + message.content + "<|end|>" }}
    {%- endif -%}
{%- endfor -%}
{#- Generation prompt #}
{%- if add_generation_prompt -%}
<|start|>assistant
{%- endif -%}
"""
stop_tokens = ["<|endoftext|>", "<|return|>", "<|call|>"]

[source]
"gpt-oss-20b.zt" = "https://huggingface.co/pie-project/gpt-oss-20b/resolve/main/gpt-oss-20b.zt"
"gpt-oss.vocab" = "https://huggingface.co/pie-project/gpt-oss-20b/resolve/main/gpt-oss.vocab"
