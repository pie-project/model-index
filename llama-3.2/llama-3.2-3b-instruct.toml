# Model metadata
name = "llama-3.2-3b-instruct"
description = "Meta Llama 3.2 3B Instruct Model"
version = "1.0"

# Model architecture details
[architecture]
type = "l4ma"
num_layers = 28
num_query_heads = 24
num_key_value_heads = 8
head_size = 128
hidden_size = 3072
intermediate_size = 8192
vocab_size = 128256

[architecture.rope]
factor = 32.0
high_frequency_factor = 4.0
low_frequency_factor = 1.0
theta = 500000.0

# Model parameters (weights)
parameters = ["llama-3.2-3b-instruct.zt"] 

# Tokenizer configuration
[tokenizer]
type = "bpe"
vocabulary_file = "llama-3.2.vocab"
split_regex = "(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"

[tokenizer.special_tokens]
"128000" = "<|begin_of_text|>"
"128001" = "<|end_of_text|>"
"128006" = "<|start_header_id|>"
"128007" = "<|end_header_id|>"
"128009" = "<|eot_id|>"

# Prompt template configuration
[template]
type = "tera"
content = """
<|start_header_id|>system<|end_header_id|>

Cutting Knowledge Date: December 2023

{% if system %}{{ system }}
{% endif %}
{% if tools %}When you receive a tool call response, use the output to format an answer to the orginal user question.

You are a helpful assistant with tool calling capabilities.
{% endif %}<|eot_id|>
{% for message in messages %}
{% set last = loop.last %}
{% if message.role == "user" %}<|start_header_id|>user<|end_header_id|>
{% if tools and last %}

Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.

Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}. Do not use variables.

{% for tool in tools %}
{{ tool }}
{% endfor %}
{{ message.content }}<|eot_id|>
{% else %}

{{ message.content }}<|eot_id|>
{% endif %}{% if last %}<|start_header_id|>assistant<|end_header_id|>

{% endif %}
{% elif message.role == "assistant" %}<|start_header_id|>assistant<|end_header_id|>
{% if message.tool_calls %}
{% for tool_call in message.tool_calls %}
{"name": "{{ tool_call.function.name }}", "parameters": {{ tool_call.function.arguments }}}{% endfor %}
{% else %}

{{ message.content }}
{% endif %}{% if not last %}<|eot_id|>
{% endif %}
{% elif message.role == "tool" %}<|start_header_id|>ipython<|end_header_id|>

{{ message.content }}<|eot_id|>
{% if last %}<|start_header_id|>assistant<|end_header_id|>

{% endif %}
{% endif %}
{% endfor %}
"""

# Source repositories
[[source]]
repository = "huggingface"
repository_id = "meta-llama/Llama-3.2-1B-Instruct"
revision = "1.0.0"
weight_maps = "huggingface.map"

[[source]]
repository = "ollama"
repository_id = "meta-llama/llama-3.2-1b-instruct"
revision = "1.0.0"
weight_maps = "ollama.map"