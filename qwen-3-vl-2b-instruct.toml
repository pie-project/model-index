# Model metadata
name = "qwen-3-vl-2b-instruct"
description = "Qwen3 VL 2B Instruct - Vision Language Model"
version = "1.0"
parameters = ["qwen-3-vl-2b-instruct.zt"]

# Model architecture (from config.json)
[architecture]
type = "qwen3_vl"
# Text config
num_layers = 28
num_query_heads = 16
num_key_value_heads = 8
head_size = 128
hidden_size = 2048
intermediate_size = 6144
vocab_size = 151936
use_qkv_bias = false
rms_norm_eps = 1e-6
max_position_embeddings = 262144
hidden_act = "silu"
attention_dropout = 0.0
bos_token_id = 151643
eos_token_id = 151645

[architecture.rope]
theta = 5000000.0
type = "mrope"
mrope_interleaved = true
mrope_section = [24, 20, 20]

# Vision configuration (from config.json -> vision_config)
[architecture.vision]
model_type = "qwen3_vl"
depth = 24
hidden_size = 1024
out_hidden_size = 2048
num_heads = 16
patch_size = 16
spatial_merge_size = 2
temporal_patch_size = 2
in_channels = 3
intermediate_size = 4096
hidden_act = "gelu_pytorch_tanh"
num_position_embeddings = 2304
deepstack_visual_indexes = [5, 11, 17]

# Token IDs (from config.json)
image_token_id = 151655
video_token_id = 151656
vision_start_token_id = 151652
vision_end_token_id = 151653

# Tokenizer (from tokenizer_config.json)
[tokenizer]
type = "bpe"
vocabulary_file = "qwen-3-vl.vocab"
split_regex = "(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}++|\\p{N}{1,3}+| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*+|\\s++$|\\s*[\\r\\n]|\\s+(?!\\S)|\\s"
escape_non_printable = true

# Special tokens (from tokenizer_config.json -> added_tokens_decoder)
[tokenizer.special_tokens]
"<|endoftext|>" = 151643
"<|im_start|>" = 151644
"<|im_end|>" = 151645
"<|object_ref_start|>" = 151646
"<|object_ref_end|>" = 151647
"<|box_start|>" = 151648
"<|box_end|>" = 151649
"<|quad_start|>" = 151650
"<|quad_end|>" = 151651
"<|vision_start|>" = 151652
"<|vision_end|>" = 151653
"<|vision_pad|>" = 151654
"<|image_pad|>" = 151655
"<|video_pad|>" = 151656
"<tool_call>" = 151657
"</tool_call>" = 151658
"<|fim_prefix|>" = 151659
"<|fim_middle|>" = 151660
"<|fim_suffix|>" = 151661
"<|fim_pad|>" = 151662
"<|repo_name|>" = 151663
"<|file_sep|>" = 151664
"<tool_response>" = 151665
"</tool_response>" = 151666
"<think>" = 151667
"</think>" = 151668

# Prompt template (simplified from tokenizer_config.json -> chat_template)
[template]
type = "minijinja"
content = """
{%- for m in messages %}
{%- if m.role == "user" %}
<|im_start|>user
{{ m.content }}<|im_end|>
{%- elif m.role == "system" %}
<|im_start|>system
{{ m.content }}<|im_end|>
{%- elif m.role == "assistant" %}
<|im_start|>assistant
{{ m.content }}<|im_end|>
{%- endif %}
{%- endfor %}
{%- if add_generation_prompt %}
<|im_start|>assistant
{%- endif %}
"""
stop_tokens = ["<|im_end|>", "<|im_start|>", "<|endoftext|>"]

# Source URLs
[source]
"qwen-3-vl-2b-instruct.zt" = "https://huggingface.co/pie-project/qwen-3-vl-2b-instruct/resolve/main/qwen-3-vl-2b-instruct.zt"
"qwen-3-vl.vocab" = "https://huggingface.co/pie-project/qwen-3-vl-2b-instruct/resolve/main/qwen-3-vl.vocab"
