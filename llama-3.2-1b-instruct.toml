# Model metadata
name = "llama-3.2-1b-instruct"
description = "Meta Llama 3.2 1B Instruct Model"
version = "1.0"
parameters = ["llama-3.2-1b-instruct.zt"] 

# Model architecture details
[architecture]
type = "l4ma"
num_layers = 16
num_query_heads = 32
num_key_value_heads = 8
head_size = 64
hidden_size = 2048
intermediate_size = 8192
vocab_size = 128256

[architecture.rope]
factor = 32.0
high_frequency_factor = 4.0
low_frequency_factor = 1.0
theta = 500000.0

# Tokenizer configuration
[tokenizer]
type = "bpe"
vocabulary_file = "llama-3.2.vocab"
split_regex = "(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"

[tokenizer.special_tokens]
"<|begin_of_text|>" = 128000
"<|end_of_text|>" = 128001
"<|start_header_id|>" = 128006
"<|end_header_id|>" = 128007
"<|eot_id|>" = 128009

# Prompt template configuration
[template]
type = "tera"
content = """
<|start_header_id|>system<|end_header_id|>

Cutting Knowledge Date: December 2023

{% if system %}{{ system }}
{% endif %}
{% if tools %}When you receive a tool call response, use the output to format an answer to the orginal user question.

You are a helpful assistant with tool calling capabilities.
{% endif %}<|eot_id|>
{% for message in messages %}
{% set last = loop.last %}
{% if message.role == "user" %}<|start_header_id|>user<|end_header_id|>
{% if tools and last %}

Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.

Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}. Do not use variables.

{% for tool in tools %}
{{ tool }}
{% endfor %}
{{ message.content }}<|eot_id|>
{% else %}

{{ message.content }}<|eot_id|>
{% endif %}{% if last %}<|start_header_id|>assistant<|end_header_id|>

{% endif %}
{% elif message.role == "assistant" %}<|start_header_id|>assistant<|end_header_id|>
{% if message.tool_calls %}
{% for tool_call in message.tool_calls %}
{"name": "{{ tool_call.function.name }}", "parameters": {{ tool_call.function.arguments }}}{% endfor %}
{% else %}

{{ message.content }}
{% endif %}{% if not last %}<|eot_id|>
{% endif %}
{% elif message.role == "tool" %}<|start_header_id|>ipython<|end_header_id|>

{{ message.content }}<|eot_id|>
{% if last %}<|start_header_id|>assistant<|end_header_id|>

{% endif %}
{% endif %}
{% endfor %}
"""

[source]
"llama-3.2-1b-instruct.zt" = "https://huggingface.co/pie-project/llama-3.2-1b-instruct/resolve/main/llama-3.2-1b-instruct.zt"
"llama-3.2.vocab" = "https://huggingface.co/pie-project/llama-3.2-1b-instruct/resolve/main/llama-3.2.vocab"